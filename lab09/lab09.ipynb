{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSC 80: Lab 09\n",
    "\n",
    "### Due Date: Tuesday, December 8th, 11:59 pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "Much like in DSC 10, this Jupyter Notebook contains the statements of the problems and provides code and markdown cells to display your answers to the problems. Unlike DSC 10, the notebook is *only* for displaying a readable version of your final answers. The coding work will be developed in an accompanying `lab*.py` file, that will be imported into the current notebook.\n",
    "\n",
    "Labs and programming assignments will be graded in (at most) two ways:\n",
    "1. The functions and classes in the accompanying python file will be tested (a la DSC 20),\n",
    "2. The notebook will be graded (for graphs and free response questions).\n",
    "\n",
    "**Do not change the function names in the `*.py` file**\n",
    "- The functions in the `*.py` file are how your assignment is graded, and they are graded by their name. The dictionary at the end of the file (`GRADED FUNCTIONS`) contains the \"grading list\". The final function in the file allows your doctests to check that all the necessary functions exist.\n",
    "- If you changed something you weren't supposed to, just use git to revert!\n",
    "\n",
    "**Tips for working in the Notebook**:\n",
    "- The notebooks serve to present you the questions and give you a place to present your results for later review.\n",
    "- The notebook on *lab assignments* are not graded (only the `.py` file).\n",
    "- Notebooks for PAs will serve as a final report for the assignment, and contain conclusions and answers to open ended questions that are graded.\n",
    "- The notebook serves as a nice environment for 'pre-development' and experimentation before designing your function in your `.py` file.\n",
    "\n",
    "**Tips for developing in the .py file**:\n",
    "- Do not change the function names in the starter code; grading is done using these function names.\n",
    "- Do not change the docstrings in the functions. These are there to tell you if your work is on the right track!\n",
    "- You are encouraged to write your own additional functions to solve the lab! \n",
    "    - Developing in python usually consists of larger files, with many short functions.\n",
    "    - You may write your other functions in an additional `.py` file that you import in `lab**.py` (much like we do in the notebook).\n",
    "- Always document your code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing code from `lab**.py`\n",
    "\n",
    "* We import our `.py` file that's contained in the same directory as this notebook.\n",
    "* We use the `autoreload` notebook extension to make changes to our `lab**.py` file immediately available in our notebook. Without this extension, we would need to restart the notebook kernel to see any changes to `lab**.py` in the notebook.\n",
    "    - `autoreload` is necessary because, upon import, `lab**.py` is compiled to bytecode (in the directory `__pycache__`). Subsequent imports of `lab**` merely import the existing compiled python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import lab09 as lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Acquainted with `sklearn` Pipelines\n",
    "\n",
    "The file `data/toy.csv` contains toy data that consists of 4 columns:\n",
    "```\n",
    "group: a categorical column with 3 categories\n",
    "c1: a numeric attribute\n",
    "c2: a numeric attribute\n",
    "y: the target variable (that you want to predict) \n",
    "```\n",
    "\n",
    "In the following questions, you will build ML pipelines that combine feature engineering with a simple linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>4.058118</td>\n",
       "      <td>5.329582</td>\n",
       "      <td>12.649035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>4.194945</td>\n",
       "      <td>3.839476</td>\n",
       "      <td>17.309083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>2.246411</td>\n",
       "      <td>10.694666</td>\n",
       "      <td>15.695646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>2.510912</td>\n",
       "      <td>6.414960</td>\n",
       "      <td>11.535752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B</td>\n",
       "      <td>3.194722</td>\n",
       "      <td>6.116839</td>\n",
       "      <td>14.954389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  group        c1         c2          y\n",
       "0     B  4.058118   5.329582  12.649035\n",
       "1     A  4.194945   3.839476  17.309083\n",
       "2     A  2.246411  10.694666  15.695646\n",
       "3     B  2.510912   6.414960  11.535752\n",
       "4     B  3.194722   6.116839  14.954389"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp = os.path.join('data', 'toy.csv')\n",
    "data = pd.read_csv(fp)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1**\n",
    "\n",
    "First, you will train a regression model using only a *log-scaled* `c2` variable. Create a simple pipeline that:\n",
    "1. log-scales `c2`, then\n",
    "2. predicts `y` using a linear regression model (using your transformed `c2`).\n",
    "\n",
    "That is, create a function `simple_pipeline` that takes in a dataframe like `data` and returns a tuple consisting of the pipeline and the predictions your model makes on `data` (as trained on `data`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def simple_pipeline(data):\n",
    "    '''\n",
    "    simple_pipeline takes in a dataframe like data and returns a tuple \n",
    "    consisting of the pipeline and the predictions your model makes \n",
    "    on data (as trained on data).\n",
    "\n",
    "    :Example:\n",
    "    >>> fp = os.path.join('data', 'toy.csv')\n",
    "    >>> data = pd.read_csv(fp)\n",
    "    >>> pl, preds = simple_pipeline(data)\n",
    "    >>> isinstance(pl, Pipeline)\n",
    "    True\n",
    "    >>> isinstance(pl.steps[-1][1], LinearRegression)\n",
    "    True\n",
    "    >>> isinstance(pl.steps[0][1], FunctionTransformer)\n",
    "    True\n",
    "    >>> preds.shape[0] == data.shape[0]\n",
    "    True\n",
    "    '''\n",
    "    steps = [('log_c',FunctionTransformer(np.log)),('lr',LinearRegression())]\n",
    "    pl = Pipeline(steps)\n",
    "    pl.fit(data[['c2']],data['y'])\n",
    "    preds = pl.predict(data[['c2']])\n",
    "    return tuple([pl,preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fp = os.path.join('data', 'toy.csv')\n",
    "data = pd.read_csv(fp)\n",
    "pl, preds = simple_pipeline(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(pl.steps[-1][1], LinearRegression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2**\n",
    "\n",
    "Now, you will engineer features from the other columns and use them to train a regression model.  Create a pipeline that:\n",
    "1. uses `c1` as is,\n",
    "1. log-scales `c2`,\n",
    "1. one-hot encodes `group`,\n",
    "1. predicts `y` using a linear regression model built on the three variable-classes above.\n",
    "\n",
    "Use `ColumnTransformer` to put together all the column-specific preprocessing steps into a single set of features for fitting the regression model.\n",
    "\n",
    "That is, create a function `multi_type_pipeline` that takes in a dataframe like `data` and returns a tuple consisting of the pipeline and the predictions your model makes on `data` (as trained on `data`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def multi_type_pipeline(data):\n",
    "    '''\n",
    "    multi_type_pipeline that takes in a dataframe like data and \n",
    "    returns a tuple consisting of the pipeline and the predictions \n",
    "    your model makes on data (as trained on data).\n",
    "\n",
    "    :Example:\n",
    "    >>> fp = os.path.join('data', 'toy.csv')\n",
    "    >>> data = pd.read_csv(fp)\n",
    "    >>> pl, preds = multi_type_pipeline(data)\n",
    "    >>> isinstance(pl, Pipeline)\n",
    "    True\n",
    "    >>> isinstance(pl.steps[-1][1], LinearRegression)\n",
    "    True\n",
    "    >>> isinstance(pl.steps[0][1], ColumnTransformer)\n",
    "    True\n",
    "    >>> data.shape[0] == preds.shape[0]\n",
    "    True\n",
    "    '''\n",
    "    steps = [('log_c',FunctionTransformer(np.log),['c2']),('one_hot',OneHotEncoder(),['group'])]\n",
    "    c_trans = ('col_trans',ColumnTransformer(steps))\n",
    "    pl = Pipeline([c_trans,('lr',LinearRegression())])\n",
    "    pl.fit(data[['c1','c2','group']],data['y'])\n",
    "    preds = pl.predict(data[['c1','c2','group']])\n",
    "    return tuple([pl,preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fp = os.path.join('data', 'toy.csv')\n",
    "data = pd.read_csv(fp)\n",
    "pl, preds = multi_type_pipeline(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3**\n",
    "\n",
    "Notice that `c1` and `c2` have strong associations with the values of `group` (e.g. try `sns.scatterplot` of `c1` and `y`, with `hue='group'`). This suggests that group-wise scaling might make good features. \n",
    "\n",
    "In this question, you will `z-scale` both `c1` and `c2` *within* each group `A,B,C`. However, `sklearn` doesn't include this transformer, so it will be necessary to *create your own*. In the starter code, is a skeleton for creating your own `sklearn` transformer class; you will need to create your own `transformer` and `fit` methods.\n",
    "\n",
    "* You will create `StdScalerByGroup` that fits/transforms input data `X` whose first column contains the groups; the other columns are numeric and will be z-scaled within each group.\n",
    "* The `fit` method should determine the mean and sample standard-deviation of each group value in the input data `X` and save them in the instance variable `grps_`.\n",
    "* The `transform` method takes in data `X` and z-scales the columns using the mean/std saved in `grps_`.\n",
    "\n",
    "*Note:* You may decide on whatever structure you'd like for the `grps_` variable. This question will be graded on the correctness of the output. (Check the correctness of your work by checking the output by-hand!)\n",
    "\n",
    "*Note:* A reminder that avoid using loops over long iterable subjects!\n",
    "\n",
    "*Note:* The `group` column in the doctest is named 'g' instead of 'group'. Remember, the first column will **always** contain the groups (even if the name is different)\n",
    "\n",
    "*Note:* Do not worry about cases where the std = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class StdScalerByGroup(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        :Example:\n",
    "        >>> cols = {'g': ['A', 'A', 'B', 'B'], 'c1': [1, 2, 2, 2], 'c2': [3, 1, 2, 0]}\n",
    "        >>> X = pd.DataFrame(cols)\n",
    "        >>> std = StdScalerByGroup().fit(X)\n",
    "        >>> std.grps_ is not None\n",
    "        True\n",
    "        \"\"\"\n",
    "        # X may not be a pandas dataframe (e.g. a np.array)\n",
    "        df = pd.DataFrame(X)\n",
    "        means = df.groupby(X.iloc[:,0]).mean().to_dict()\n",
    "        stds = df.groupby(X.iloc[:,0]).std().to_dict()\n",
    "        # A dictionary of means/standard-deviations for each column, for each group.\n",
    "        \n",
    "        self.grps_ = (means, stds)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        :Example:\n",
    "        >>> cols = {'g': ['A', 'A', 'B', 'B'], 'c1': [1, 2, 3, 4], 'c2': [1, 2, 3, 4]}\n",
    "        >>> X = pd.DataFrame(cols)\n",
    "        >>> std = StdScalerByGroup().fit(X)\n",
    "        >>> out = std.transform(X)\n",
    "        >>> out.shape == (4, 2)\n",
    "        True\n",
    "        >>> np.isclose(out.abs(), 0.707107, atol=0.001).all().all()\n",
    "        True\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            getattr(self, \"grps_\")\n",
    "        except AttributeError:\n",
    "            raise RuntimeError(\"You must fit the transformer before tranforming the data!\")\n",
    "        \n",
    "\n",
    "        # Define a helper function here?\n",
    "        def normalize(row):\n",
    "            out_row = []\n",
    "            for i in range(len(row) - 1):\n",
    "                if self.grps_[1][df.columns[i+1]][row[0]] == 0:\n",
    "                    out_row.append(0)\n",
    "                else:\n",
    "                    out_row.append((row[i + 1] - self.grps_[0][df.columns[i+1]][row[0]]) / self.grps_[1][df.columns[i+1]][row[0]])\n",
    "            \n",
    "            return pd.Series(out_row)\n",
    "        # X may not be a dataframe (e.g. np.array)\n",
    "\n",
    "        df = pd.DataFrame(X)\n",
    "        out_df = df.apply(normalize, axis = 1)\n",
    "        for i in range(len(df.columns) - 1):\n",
    "            out_df = out_df.rename(columns = {i:df.columns[i + 1]})\n",
    "        return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = {'g': ['A', 'A', 'B', 'B'], 'c1': [1, 2, 3, 4], 'c2': [1, 2, 3, 4]}\n",
    "X = pd.DataFrame(cols)\n",
    "fp = os.path.join('data', 'toy.csv')\n",
    "data = pd.read_csv(fp)\n",
    "std = StdScalerByGroup().fit(X)\n",
    "out = std.transform(X)\n",
    "out\n",
    "np.isclose(out.abs(), 0.707107, atol=0.001).all().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'c1': {'A': 1.5, 'B': 2.0}, 'c2': {'A': 2.0, 'B': 1.0}},\n",
       " {'c1': {'A': 0.7071067811865476, 'B': 0.0},\n",
       "  'c2': {'A': 1.4142135623730951, 'B': 1.4142135623730951}})"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std.grps_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False],\n",
       "       [False, False],\n",
       "       [False, False],\n",
       "       [False, False]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isclose(out.abs(), 0.707107, atol=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4**\n",
    "\n",
    "Pipelines are supposed to help you easily try different model configurations. Create a function `eval_toy_model` which returns a hard-coded list of tuples consisting of the (RMSE, $R^2$) of 3 different modeling pipelines (fit and evaluated on the entire input `data`). The three different models are:\n",
    "1. The pipeline in Question 1\n",
    "1. The pipeline in Question 2\n",
    "1. A pipeline consisting of a linear regression model fit on features generated by applying `StdScalerByGroup` to `c1`, log-scaling `c2`, and applying `OneHotEncoder` to `group`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def eval_toy_model():\n",
    "    \"\"\"\n",
    "    hardcoded answers to question 4\n",
    "\n",
    "    :Example:\n",
    "    >>> out = eval_toy_model()\n",
    "    >>> len(out) == 3\n",
    "    True\n",
    "    \"\"\"\n",
    "    \n",
    "    return (7.598222156931544,5.536928354075744,5.363818490270249)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.598222156931544"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((data['y'] - simple_pipeline(data)[1]) ** 2).sum() / (data.shape[0] - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.536928354075744"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((data['y'] - multi_type_pipeline(data)[1]) ** 2).sum() / (data.shape[0] - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_multi_type_pipeline(data):\n",
    "    \n",
    "    steps = [('std_scale',StdScalerByGroup(),['c1']),('log_c',FunctionTransformer(np.log),['c2']),('one_hot',OneHotEncoder(),['group'])]\n",
    "    c_trans = ('col_trans',ColumnTransformer(steps))\n",
    "    pl = Pipeline([c_trans,('lr',LinearRegression())])\n",
    "    pl.fit(data[['c1','c2','group']],data['y'])\n",
    "    preds = pl.predict(data[['c1','c2','group']])\n",
    "    return tuple([pl,preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.363818490270249"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((data['y'] - temp_multi_type_pipeline(data)[1]) ** 2).sum() / (data.shape[0] - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting model parameters\n",
    "\n",
    "**Question 5**\n",
    "\n",
    "In this question, you will train two different prediction models on Galton's child-height dataset from lecture and explore different ways in how overfitting can appear.\n",
    "\n",
    "**Part 1: Decision Tree Regressor**\n",
    "\n",
    "* A decision tree regressor is trained similar to decision tree classifiers: the splits of the tree are created by minimizing the variance of the target values of the (training) data in the leaves given by making the split in question. \n",
    "\n",
    "* A decision tree regressor predicts the target value of a (new) observation based on the average target value of the training observations lying in the same leaf node. \n",
    "\n",
    "* One parameter of a decision tree regressor that affects model complexity is the *depth* of the tree. We will explore this parameter in this question.\n",
    "\n",
    "* Create a function `tree_reg_perf` that takes in a dataframe like `galton` and outputs a dataframe where each row contains the *RMSE* of a trained decision tree regressor on the training set and test set, indexed by the depth of the decision tree (depth=1,2,3,...,20). (i.e. you should train 20 different decision trees with varying depths and put the train/test error into a dataframe).\n",
    "\n",
    "*Note* (Optional question good for studying): How is this overfitting and why? What type of variance is causing it? What is the best choice of depth? Plot the dataframe above to help answer these questions.\n",
    "\n",
    "**Part 2: k-Nearest Neighbor Regressor**\n",
    "\n",
    "* A k-NN Regressor predicts the target value of a (new) observation by computing the average value of the k-closest observations in the training set.\n",
    "\n",
    "* One parameter of a k-NN regressor that affects model performance is the number of neighbors averaged over. We will explore this parameter in this question.\n",
    "\n",
    "* Create a function `knn_reg_perf` that takes in a dataframe like `galton` and outputs a dataframe where each row contains the *RMSE* of a trained k-NN regressor on the training set and test set, indexed by the number of neighbors (k=1,2,3,...,20).\n",
    "\n",
    "*Note* (Optional question good for studying): How is this overfitting and why? What type of variance is causing it? What is the best choice for the number of neighbors? Plot the dataframe above to help answer these questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def tree_reg_perf(galton):\n",
    "    \"\"\"\n",
    "\n",
    "    :Example:\n",
    "    >>> galton_fp = os.path.join('data', 'galton.csv')\n",
    "    >>> galton = pd.read_csv(galton_fp)\n",
    "    >>> out = tree_reg_perf(galton)\n",
    "    >>> out.columns.tolist() == ['train_err', 'test_err']\n",
    "    True\n",
    "    >>> out['train_err'].iloc[-1] < out['test_err'].iloc[-1]\n",
    "    True\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(galton.drop(columns = 'childHeight'),galton['childHeight'])\n",
    "    out = pd.DataFrame(columns = ['train_err','test_err'])\n",
    "    for i in range(20):\n",
    "        \n",
    "        tree = DecisionTreeRegressor(max_depth = i + 1)\n",
    "        tree.fit(X_train, y_train)\n",
    "        train_pred = tree.predict(X_train)\n",
    "        test_pred = tree.predict(X_test)\n",
    "        train_r = ((y_train - train_pred) ** 2).sum() / (len(y_train) - 1)\n",
    "        test_r = ((y_test - test_pred) ** 2).sum() / (len(y_test) - 1)\n",
    "        temp = pd.DataFrame({'train_err':[train_r], 'test_err':[test_r]}).set_index(pd.Series([i + 1]))\n",
    "        out = pd.concat([out,temp])\n",
    "    return out\n",
    "\n",
    "\n",
    "def knn_reg_perf(galton):\n",
    "    \"\"\"\n",
    "    :Example:\n",
    "    >>> galton_fp = os.path.join('data', 'galton.csv')\n",
    "    >>> galton = pd.read_csv(galton_fp)\n",
    "    >>> out = knn_reg_perf(galton)\n",
    "    >>> out.columns.tolist() == ['train_err', 'test_err']\n",
    "    True\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(galton.drop(columns = 'childHeight'),galton['childHeight'])\n",
    "    out = pd.DataFrame(columns = ['train_err','test_err'])\n",
    "    for i in range(20):\n",
    "        \n",
    "        tree = KNeighborsRegressor(n_neighbors = i + 1)\n",
    "        tree.fit(X_train, y_train)\n",
    "        train_pred = tree.predict(X_train)\n",
    "        test_pred = tree.predict(X_test)\n",
    "        train_r = ((y_train - train_pred) ** 2).sum() / (len(y_train) - 1)\n",
    "        test_r = ((y_test - test_pred) ** 2).sum() / (len(y_test) - 1)\n",
    "        temp = pd.DataFrame({'train_err':[train_r], 'test_err':[test_r]}).set_index(pd.Series([i + 1]))\n",
    "        out = pd.concat([out,temp])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_err</th>\n",
       "      <th>test_err</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.273337</td>\n",
       "      <td>6.154023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.351036</td>\n",
       "      <td>5.565382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.609464</td>\n",
       "      <td>5.000785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.073577</td>\n",
       "      <td>4.541827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.491875</td>\n",
       "      <td>4.221051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.980906</td>\n",
       "      <td>3.999866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.460880</td>\n",
       "      <td>4.411516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.981563</td>\n",
       "      <td>5.194370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.480225</td>\n",
       "      <td>5.145739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.075381</td>\n",
       "      <td>5.747399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.675964</td>\n",
       "      <td>5.905869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.348815</td>\n",
       "      <td>6.711272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.203366</td>\n",
       "      <td>6.588124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.123300</td>\n",
       "      <td>6.775125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.104986</td>\n",
       "      <td>6.430000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.089306</td>\n",
       "      <td>6.823562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.087361</td>\n",
       "      <td>6.546738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.087182</td>\n",
       "      <td>6.455097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.087182</td>\n",
       "      <td>6.699871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.087182</td>\n",
       "      <td>6.941964</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    train_err  test_err\n",
       "1    6.273337  6.154023\n",
       "2    5.351036  5.565382\n",
       "3    4.609464  5.000785\n",
       "4    4.073577  4.541827\n",
       "5    3.491875  4.221051\n",
       "6    2.980906  3.999866\n",
       "7    2.460880  4.411516\n",
       "8    1.981563  5.194370\n",
       "9    1.480225  5.145739\n",
       "10   1.075381  5.747399\n",
       "11   0.675964  5.905869\n",
       "12   0.348815  6.711272\n",
       "13   0.203366  6.588124\n",
       "14   0.123300  6.775125\n",
       "15   0.104986  6.430000\n",
       "16   0.089306  6.823562\n",
       "17   0.087361  6.546738\n",
       "18   0.087182  6.455097\n",
       "19   0.087182  6.699871\n",
       "20   0.087182  6.941964"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "galton_fp = os.path.join('data', 'galton.csv')\n",
    "galton = pd.read_csv(galton_fp)\n",
    "out = tree_reg_perf(galton)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_err</th>\n",
       "      <th>test_err</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.173104</td>\n",
       "      <td>6.844764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.708988</td>\n",
       "      <td>5.287650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.305503</td>\n",
       "      <td>4.598321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.775437</td>\n",
       "      <td>4.440467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.112303</td>\n",
       "      <td>4.206299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.281486</td>\n",
       "      <td>4.188542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.563599</td>\n",
       "      <td>4.333980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.696115</td>\n",
       "      <td>4.447496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.854015</td>\n",
       "      <td>4.414612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4.022934</td>\n",
       "      <td>4.458491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.139030</td>\n",
       "      <td>4.447945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.219047</td>\n",
       "      <td>4.397389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.312731</td>\n",
       "      <td>4.493597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4.373397</td>\n",
       "      <td>4.512963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4.494923</td>\n",
       "      <td>4.483745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4.534556</td>\n",
       "      <td>4.469041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4.572151</td>\n",
       "      <td>4.480051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4.602608</td>\n",
       "      <td>4.467805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4.647350</td>\n",
       "      <td>4.489347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4.713782</td>\n",
       "      <td>4.473695</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    train_err  test_err\n",
       "1    0.173104  6.844764\n",
       "2    1.708988  5.287650\n",
       "3    2.305503  4.598321\n",
       "4    2.775437  4.440467\n",
       "5    3.112303  4.206299\n",
       "6    3.281486  4.188542\n",
       "7    3.563599  4.333980\n",
       "8    3.696115  4.447496\n",
       "9    3.854015  4.414612\n",
       "10   4.022934  4.458491\n",
       "11   4.139030  4.447945\n",
       "12   4.219047  4.397389\n",
       "13   4.312731  4.493597\n",
       "14   4.373397  4.512963\n",
       "15   4.494923  4.483745\n",
       "16   4.534556  4.469041\n",
       "17   4.572151  4.480051\n",
       "18   4.602608  4.467805\n",
       "19   4.647350  4.489347\n",
       "20   4.713782  4.473695"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "galton_fp = os.path.join('data', 'galton.csv')\n",
    "galton = pd.read_csv(galton_fp)\n",
    "out = knn_reg_perf(galton)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titanic: predicting survival\n",
    "\n",
    "**Question 6**\n",
    "\n",
    "Predicting survival on the titanic is a common first assignment when learning classification tasks. There is *a lot* of material out there on analyzing the Titanic data. While not necessary for this question, you are encouraged to look at examples out on the web (e.g. [on kaggle](https://www.kaggle.com/c/titanic)).\n",
    "\n",
    "Create a function `titanic_model` that takes in training data (a dataframe) and returns a pipeline object fit to the training data. You have freedom to build your own model, but it should satisfy the following requirements:\n",
    "\n",
    "* The model is built on the target column `Survived`.\n",
    "* The model uses features derived from *all* **other** columns in training.\n",
    "* You have one feature derived from the 'title' in the `Name` field ('Mr', 'Miss', 'Master', etc, not the name itself).\n",
    "* You have one feature that scales the age of a passenger to standard unit among their `Pclass`. (Use Question 3!)\n",
    "* You can use any classification algorithm, as long as your performance is above the baseline accuracy 0.78.\n",
    "* If your model (under retraining) can consistently score above 0.83, you can get 5 points extra-credit.\n",
    "    - Your model will be trained using the training data given in the lab; it will be evaluated using held out test data. This performance should generalize to *unseen* data; you should be honest about parameter-fitting on validation sets and get a score above 0.83 on an untouched *holdout*!\n",
    "\n",
    "*Note:* You will find [FunctionTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html) useful for the `Name` column, as well as for the standardization of `Ages`. If you want your transformer to output a categorical feature, you will need to select `validate=False`.\n",
    "\n",
    "*Note*: When using [ColumnTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html#sklearn.compose.ColumnTransformer), you may find the `remainder` keyword helpful.\n",
    "\n",
    "*Note*: The function that is turned in should have the model parameters hard-coded in it. The pipeline object doesn't have to include the parameter selection process.\n",
    "\n",
    "*Hint*: If you are set out to get that extra 5 points, consider building some meaningful features before fine-tuning the parameters of your model. Do an EDA on the dataset, what kind of people are more prone to survive?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "def titanic_model(titanic):\n",
    "    \"\"\"\n",
    "    :Example:\n",
    "    >>> fp = os.path.join('data', 'titanic.csv')\n",
    "    >>> data = pd.read_csv(fp)\n",
    "    >>> pl = titanic_model(data)\n",
    "    >>> isinstance(pl, Pipeline)\n",
    "    True\n",
    "    >>> from sklearn.base import BaseEstimator\n",
    "    >>> isinstance(pl.steps[-1][-1], BaseEstimator)\n",
    "    True\n",
    "    >>> preds = pl.predict(data.drop('Survived', axis=1))\n",
    "    >>> ((preds == 0)|(preds == 1)).all()\n",
    "    True\n",
    "    \"\"\"\n",
    "    data = titanic.copy()\n",
    "    def get_title(name):\n",
    "        title = name.split(' ')[0]\n",
    "        if title not in ['Mr.','Mrs.','Miss.','Master.','Dr.']:\n",
    "            title = 'other'\n",
    "        return title\n",
    "    def get_titles(names):\n",
    "        return pd.DataFrame(names.iloc[:,0].apply(get_title))\n",
    "    title_transformer = Pipeline([('get_title',FunctionTransformer(func = get_titles, validate = False)),('one_hot',OneHotEncoder())])\n",
    "    step = ('std', StdScalerByGroup(), ['Pclass','Age'])\n",
    "    steps = [step,('title',title_transformer,['Name']), ('one_hot',OneHotEncoder(),['Sex'])]\n",
    "    \n",
    "    c_trans = ('col_trans',ColumnTransformer(steps, remainder = 'passthrough'))\n",
    "    pl = Pipeline([c_trans,('lr',DecisionTreeRegressor())])\n",
    "    X = data.drop(columns = 'Survived')\n",
    "    y = data['Survived']\n",
    "    pl.fit(X,y)\n",
    "    return pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('col_trans',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('std', StdScalerByGroup(),\n",
       "                                                  ['Pclass', 'Age']),\n",
       "                                                 ('title',\n",
       "                                                  Pipeline(steps=[('get_title',\n",
       "                                                                   FunctionTransformer(func=<function titanic_model.<locals>.get_titles at 0x000001846F1418B0>)),\n",
       "                                                                  ('one_hot',\n",
       "                                                                   OneHotEncoder())]),\n",
       "                                                  ['Name']),\n",
       "                                                 ('one_hot', OneHotEncoder(),\n",
       "                                                  ['Sex'])])),\n",
       "                ('lr', DecisionTreeRegressor())])"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp = os.path.join('data', 'titanic.csv')\n",
    "data = pd.read_csv(fp)\n",
    "pl = titanic_model(data)\n",
    "pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pl.predict(data.drop('Survived', axis=1))\n",
    "#((preds == 0)|(preds == 1)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9761946672701821"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data.drop(columns = 'Survived')\n",
    "y = data['Survived']\n",
    "pl.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexingError",
     "evalue": "Too many indexers",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-474-a8d8e7d60c28>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_titles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-473-52502f739f9a>\u001b[0m in \u001b[0;36mget_titles\u001b[1;34m(names)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_titles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_title\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1760\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1761\u001b[0m                     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1762\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1763\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1764\u001b[0m             \u001b[1;31m# we by definition only have the 0th axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   2065\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2066\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2067\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_has_valid_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2068\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2069\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_has_valid_tuple\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    699\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Too many indexers\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexingError\u001b[0m: Too many indexers"
     ]
    }
   ],
   "source": [
    "get_titles(data['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name\n",
       "Dr.          7\n",
       "Master.     29\n",
       "Miss.      151\n",
       "Mr.        409\n",
       "Mrs.        98\n",
       "other       16\n",
       "dtype: int64"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_title(name):\n",
    "        title = name.split(' ')[0]\n",
    "        if title not in ['Mr.','Mrs.','Miss.','Master.','Dr.']:\n",
    "            title = 'other'\n",
    "        return title\n",
    "data[['Name']].applymap(get_title).groupby('Name').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Amazon Review Rating Classification\n",
    "#### You won't be graded on this problem, but it is a good practice! \n",
    "\n",
    "\n",
    "One of the problems that we often solve in NLP is a classification problem: given a set of documents and their labels (spam/not spam, positive/negative etc), one needs to build a model that can predicts the correct label on a new document. In order to apply classifiers (like random forest or Naive Bayes) we need to transform the text into a (set of) feature vector(s). We already know one of the ways it can be done: using a `bag of words`. In short, you take the available words in a text and keep count when they appear. \n",
    "\n",
    "### Bigrams and Trigrams\n",
    "\n",
    "In this question, you will build features out of bigrams/trigrams in text-documents, and use these for prediction. You will develop a general approach to a text classification problem and then use different settings (text preprocessing, different classification algorithms etc) to improve the learning rates.\n",
    "\n",
    "* You are given a data file (`reviews.json`) that you will use to classify Amazon reviews. First you need to extract the text and the rating for each review; Also you need to clean each review before building a model: (convert everything to a lower-case and replace everything but letters, numbers and spaces with a space. In order to do that write a function `json_reader` that takes a file and the number of lines you want to read from the file. It returns two lists: one with cleaned reviews and another one with corresponding ratings (labels). \n",
    "\n",
    "* Now write a function `create_classifier_multi` that takes reviews and labels and returns a fit pipeline. You should reserve 20% of you training data for testing. Then set up a classifier using a Pipeline object such that it gives you the highest possible accuracy. Here is a trick: the accuracy is not known beforehand and you should try different classifiers and change their parameters, different pre-processing steps (for example: ngrams, stop words etc) in order to maximize the classifier's score. What is the highest value that you got? \n",
    "\n",
    "Do not be discouraged if you can't get a very high accuracy. Think why might it be the case? What if we do not use any algorithms and just assign labels to a new review randomly. What is the chance that you guessed the label correctly? Your accuracy does not seem that bad anymore, right? (If it is below a random assignment then change your code NOW).\n",
    "\n",
    "In general, a multi-class classification problem is not an easy task. \n",
    "\n",
    "* Next you will convert the multi-class classification to the binary classification problem. In order to do that you need to write a method `to_binary` that takes in a list of labels and replaces all 1, 2, 3 reviews with a 0 and 4, 5 reviews with the 1. Build the model again by using different classifiers and pre-processing steps, by writing `create_classifier_binary` method and returning a pipeline that maximized the accuracy. Do you see the improvement? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fp = os.path.join('data', 'reviews.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! You're done!\n",
    "\n",
    "* Submit the lab on Gradescope"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
